# Lecture 13: Transformer and LLM (Part II)

**Lecturers:** Professor Song Han
**Date:** Fall 2023
**Corresponding Course Website Section:** efficientml.ai

## 1. üéØ Why It Matters for Edge AI

* **The Core Problem:** The fundamental quadratic complexity of the vanilla Transformer prevents long-context understanding and generation on resource-constrained edge devices. We need methods to approximate the attention mechanism more efficiently.
* **Edge AI Benefits:** Optimized LLMs enable running complex tasks like long-document summarization, code generation, and advanced chat interfaces directly on the user's device, maintaining both speed and user privacy.

---

## 2. üìù Key Concepts and Theory

* **Approaches for Efficient Attention:** The goal is to reduce the $O(N^2)$ dependency to something closer to linear $O(N)$:
    1.  **Sparse Attention (E.g., Longformer, BigBird):** Instead of attending to *all* tokens, restrict attention to a local window or use a fixed set of global/random tokens. This dramatically reduces computation by turning the $N \times N$ matrix into a sparse one.
    2.  **Attention Approximations (E.g., Linformer, Performer):** Use mathematical tricks (like kernel methods or low-rank factorization) to approximate the softmax-dot-product operation with a linear function, achieving **$O(N)$** complexity.
* **Layer-wise Optimization:**
    * **MLP Reduction:** The Feed-Forward Network (MLP) block in Transformers is often highly redundant. Techniques like **Pruning** (Lecture 3/4) or **Quantization** (Lecture 5/6) are applied here to reduce the model size without major accuracy loss.
* **LLM Compression Techniques (Post-Training):**
    * **Weight-Only Quantization (W8A16):** A common, simple, yet powerful technique where only the weights are quantized to INT8 (for memory reduction), but the activations/computations are left in FP16/FP32 (to preserve accuracy). This is often used for fast deployment.
    * **KV Cache Quantization/Compression:** Quantizing the stored $K$ and $V$ tensors to INT8 or even lower to save memory and increase memory bandwidth efficiency.

---

## 3. ‚öôÔ∏è Practical Implementation & Tools

* **Implementation Steps (LLM Deployment):**
    1.  **Quantize Weights:** Use post-training weight-only quantization to reduce the model size (e.g., LLaMA 7B from 14GB to 7GB or even less).
    2.  **Kernel Optimization:** Deploy using a runtime (like llama.cpp) that leverages highly optimized **INT8/FP16 matrix multiplication kernels** and specialized **KV cache management** (e.g., ring buffers, paged attention).
    3.  **Prompt Engineering (Lightweight Efficiency):** Using more effective prompting can sometimes reduce the required context length, indirectly improving efficiency without changing the model.
* **Tools for Efficient LLM Inference:**
    * **Flash Attention:** A technique that optimizes the standard attention mechanism by re-organizing the computation to minimize data movement between high-bandwidth memory (HBM) and faster on-chip SRAM, leading to significant wall-clock speedup.
    * **GGML/GGUF Format:** Quantization and format used by open-source projects for efficient CPU/GPU LLM inference.

---

## 4. ‚öñÔ∏è Trade-offs and Real-World Impact

* **Trade-off: Sparsity vs. Regularity:** Sparse attention methods reduce FLOPs but can introduce **irregular computation patterns**, requiring specialized kernels to get a real-world speedup. Linear attention methods (approximations) keep regularity but might sacrifice complexity.
* **The "Weight-Only" Advantage:** For inference on edge devices, **memory footprint** is the priority. Weight-only quantization offers a fantastic compromise: max memory reduction with minimal latency/accuracy impact.
* **Hardware Requirements:** Truly efficient LLM inference still requires a reasonably powerful processor (CPU with modern instruction sets or a discrete GPU/NPU) to handle the massive matrix multiplications.

---

## 5. üß™ Hands-on Lab Preview

* **What you will do:** Implement a basic linear attention approximation and compare its FLOPs/computation complexity against the standard attention for a long sequence. You will also use a quantization tool to convert a full-precision LLM weight file to an INT8 format.
* **Key Skill Acquired:** Analyzing and applying specialized LLM efficiency techniques, including weight quantization and complexity reduction methods, to make a large model deployable on a laptop.

