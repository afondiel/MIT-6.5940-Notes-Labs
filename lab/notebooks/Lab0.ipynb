{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MIT 6.5940 EfficientML.ai 2024 Fall: Lab 0 PyTorch Tutorial**"
      ],
      "metadata": {
        "id": "PFbUpH8elWQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will explore how to train a neural network with PyTorch."
      ],
      "metadata": {
        "id": "aoNr0MWd5e5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "yoBtxdvR5lwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first install a few packages that will be used in this tutorial:"
      ],
      "metadata": {
        "id": "0oLGv2RjLYh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchprofile 1>/dev/null"
      ],
      "metadata": {
        "id": "3r7Sl2cG7nZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then import a few libraries:"
      ],
      "metadata": {
        "id": "LYgp0au_LeAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torchprofile import profile_macs\n",
        "from torchvision.datasets import *\n",
        "from torchvision.transforms import *\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "I3uAhaCSlFrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)"
      ],
      "metadata": {
        "id": "j_l1wEdeHOlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "u7Y0sLyajGAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of\n",
        "size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size."
      ],
      "metadata": {
        "id": "VAbL_li0KPsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = {\n",
        "  \"train\": Compose([\n",
        "    RandomCrop(32, padding=4),\n",
        "    RandomHorizontalFlip(),\n",
        "    ToTensor(),\n",
        "  ]),\n",
        "  \"test\": ToTensor(),\n",
        "}\n",
        "\n",
        "dataset = {}\n",
        "for split in [\"train\", \"test\"]:\n",
        "  dataset[split] = CIFAR10(\n",
        "    root=\"data/cifar10\",\n",
        "    train=(split == \"train\"),\n",
        "    download=True,\n",
        "    transform=transforms[split],\n",
        "  )"
      ],
      "metadata": {
        "id": "Pqhy8EJSjJfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize a few images in the dataset and their corresponding class labels:"
      ],
      "metadata": {
        "id": "ft9wv-tIMUgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [[] for _ in range(10)]\n",
        "for image, label in dataset[\"test\"]:\n",
        "  if len(samples[label]) < 4:\n",
        "    samples[label].append(image)\n",
        "\n",
        "plt.figure(figsize=(20, 9))\n",
        "for index in range(40):\n",
        "  label = index % 10\n",
        "  image = samples[label][index // 10]\n",
        "\n",
        "  # Convert from CHW to HWC for visualization\n",
        "  image = image.permute(1, 2, 0)\n",
        "\n",
        "  # Convert from class index to class name\n",
        "  label = dataset[\"test\"].classes[label]\n",
        "\n",
        "  # Visualize the image\n",
        "  plt.subplot(4, 10, index + 1)\n",
        "  plt.imshow(image)\n",
        "  plt.title(label)\n",
        "  plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ofwgqYb2qsd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a neural network, we will need to feed data in batches. We create data loaders with batch size of 512:"
      ],
      "metadata": {
        "id": "jkigVqADNeIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataflow = {}\n",
        "for split in ['train', 'test']:\n",
        "  dataflow[split] = DataLoader(\n",
        "    dataset[split],\n",
        "    batch_size=512,\n",
        "    shuffle=(split == 'train'),\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "  )"
      ],
      "metadata": {
        "id": "4axnQCtnks_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can print the data type and shape from the training data loader:"
      ],
      "metadata": {
        "id": "_5G1Lf6hOLGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in dataflow[\"train\"]:\n",
        "  print(\"[inputs] dtype: {}, shape: {}\".format(inputs.dtype, inputs.shape))\n",
        "  print(\"[targets] dtype: {}, shape: {}\".format(targets.dtype, targets.shape))\n",
        "  break"
      ],
      "metadata": {
        "id": "ReP2g9pD6ppI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "sPAEVnixjwb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will use a variant of [VGG-11](https://arxiv.org/abs/1409.1556) (with fewer downsamples and a smaller classifier) as our model."
      ],
      "metadata": {
        "id": "rFr1Js3-e3rJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    layers = []\n",
        "    counts = defaultdict(int)\n",
        "\n",
        "    def add(name: str, layer: nn.Module) -> None:\n",
        "      layers.append((f\"{name}{counts[name]}\", layer))\n",
        "      counts[name] += 1\n",
        "\n",
        "    in_channels = 3\n",
        "    for x in self.ARCH:\n",
        "      if x != 'M':\n",
        "        # conv-bn-relu\n",
        "        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
        "        add(\"bn\", nn.BatchNorm2d(x))\n",
        "        add(\"relu\", nn.ReLU(True))\n",
        "        in_channels = x\n",
        "      else:\n",
        "        # maxpool\n",
        "        add(\"pool\", nn.MaxPool2d(2))\n",
        "\n",
        "    self.backbone = nn.Sequential(OrderedDict(layers))\n",
        "    self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
        "    x = self.backbone(x)\n",
        "\n",
        "    # avgpool: [N, 512, 2, 2] => [N, 512]\n",
        "    x = x.mean([2, 3])\n",
        "\n",
        "    # classifier: [N, 512] => [N, 10]\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "model = VGG().cuda()"
      ],
      "metadata": {
        "id": "SNLdS_UQjyBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its backbone is composed of eight `conv-bn-relu` blocks interleaved with four `maxpool`'s to downsample the feature map by 2^4 = 16 times:"
      ],
      "metadata": {
        "id": "x19LMKQYw0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.backbone)"
      ],
      "metadata": {
        "id": "BUWmYS2owzCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the feature map is pooled, its classifier predicts the final output with a linear layer:"
      ],
      "metadata": {
        "id": "mApr58LmyDqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.classifier)"
      ],
      "metadata": {
        "id": "S1GoSsh_yJgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this course focuses on efficiency, we will then inspect its model size and (theoretical) computation cost.\n"
      ],
      "metadata": {
        "id": "F_RcCWoQ8Kp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The model size can be estimated by the number of trainable parameters:"
      ],
      "metadata": {
        "id": "Zd4Xu-vMyz39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_params = 0\n",
        "for param in model.parameters():\n",
        "  if param.requires_grad:\n",
        "    num_params += param.numel()\n",
        "print(\"#Params:\", num_params)"
      ],
      "metadata": {
        "id": "4gTfqC0B7Uzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The computation cost can be estimated by the number of [multiply–accumulate operations (MACs)](https://en.wikipedia.org/wiki/Multiply–accumulate_operation) using [TorchProfile](https://github.com/zhijian-liu/torchprofile):"
      ],
      "metadata": {
        "id": "uAZoIKIbzLa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_macs = profile_macs(model, torch.zeros(1, 3, 32, 32).cuda())\n",
        "print(\"#MACs:\", num_macs)"
      ],
      "metadata": {
        "id": "OKVmyWCN7qpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model has 9.2M parameters and requires 606M MACs for inference. We will work together in the next few labs to improve its efficiency."
      ],
      "metadata": {
        "id": "OYkqpfejzxwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization"
      ],
      "metadata": {
        "id": "gjDsY9_KkIjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are working on a classification problem, we will apply [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) as our loss function to optimize the model:"
      ],
      "metadata": {
        "id": "oRg_5KeKLHPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "-K0DEhGKkKfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimization will be carried out using [stochastic gradient descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) with [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum):"
      ],
      "metadata": {
        "id": "3H8YniYeLIdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = SGD(\n",
        "  model.parameters(),\n",
        "  lr=0.4,\n",
        "  momentum=0.9,\n",
        "  weight_decay=5e-4,\n",
        ")"
      ],
      "metadata": {
        "id": "HXANib83LATH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The learning rate will be modulated using the following scheduler (which is adapted from [this blog series](https://myrtle.ai/learn/how-to-train-your-resnet/)):"
      ],
      "metadata": {
        "id": "v9X8SiWYLJw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "steps_per_epoch = len(dataflow[\"train\"])\n",
        "\n",
        "# Define the piecewise linear scheduler\n",
        "lr_lambda = lambda step: np.interp(\n",
        "  [step / steps_per_epoch],\n",
        "  [0, num_epochs * 0.3, num_epochs],\n",
        "  [0, 1, 0]\n",
        ")[0]\n",
        "\n",
        "# Visualize the learning rate schedule\n",
        "steps = np.arange(steps_per_epoch * num_epochs)\n",
        "plt.plot(steps, [lr_lambda(step) * 0.4 for step in steps])\n",
        "plt.xlabel(\"Number of Steps\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.grid(\"on\")\n",
        "plt.show()\n",
        "\n",
        "scheduler = LambdaLR(optimizer, lr_lambda)"
      ],
      "metadata": {
        "id": "8mJU5aw8KrVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "i2UFRbRYly50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first define the training function that optimizes the model for one epoch (*i.e.*, a pass over the training set):"
      ],
      "metadata": {
        "id": "IpHZJpjR7Wy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "  model: nn.Module,\n",
        "  dataflow: DataLoader,\n",
        "  criterion: nn.Module,\n",
        "  optimizer: Optimizer,\n",
        "  scheduler: LambdaLR,\n",
        ") -> None:\n",
        "  model.train()\n",
        "\n",
        "  for inputs, targets in tqdm(dataflow, desc='train', leave=False):\n",
        "    # Move the data from CPU to GPU\n",
        "    inputs = inputs.cuda()\n",
        "    targets = targets.cuda()\n",
        "\n",
        "    # Reset the gradients (from the last iteration)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward inference\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # Backward propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update optimizer and LR scheduler\n",
        "    optimizer.step()\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "79GKx_oVl09b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define the evaluation function that calculates the metric (*i.e.*, accuracy in our case) on the test set:"
      ],
      "metadata": {
        "id": "SwAbMdUq7YrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def evaluate(\n",
        "  model: nn.Module,\n",
        "  dataflow: DataLoader\n",
        ") -> float:\n",
        "  model.eval()\n",
        "\n",
        "  num_samples = 0\n",
        "  num_correct = 0\n",
        "\n",
        "  for inputs, targets in tqdm(dataflow, desc=\"eval\", leave=False):\n",
        "    # Move the data from CPU to GPU\n",
        "    inputs = inputs.cuda()\n",
        "    targets = targets.cuda()\n",
        "\n",
        "    # Inference\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Convert logits to class indices\n",
        "    outputs = outputs.argmax(dim=1)\n",
        "\n",
        "    # Update metrics\n",
        "    num_samples += targets.size(0)\n",
        "    num_correct += (outputs == targets).sum()\n",
        "\n",
        "  return (num_correct / num_samples * 100).item()"
      ],
      "metadata": {
        "id": "44_AriMP4G_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With training and evaluation functions, we can finally start training the model! This will take around 10 minutes."
      ],
      "metadata": {
        "id": "6XSU9oFD7aXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch_num in tqdm(range(1, num_epochs + 1)):\n",
        "  train(model, dataflow[\"train\"], criterion, optimizer, scheduler)\n",
        "  metric = evaluate(model, dataflow[\"test\"])\n",
        "  print(f\"epoch {epoch_num}:\", metric)"
      ],
      "metadata": {
        "id": "4iWaYpw_4E8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything goes well, your trained model should be able to achieve >92.5\\% of accuracy!"
      ],
      "metadata": {
        "id": "96XTuBat-e-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "ck6iME0rLjuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the model's prediction to see how the model truly performs:"
      ],
      "metadata": {
        "id": "-8rVV1SOSUsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "for index in range(40):\n",
        "  image, label = dataset[\"test\"][index]\n",
        "\n",
        "  # Model inference\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    pred = model(image.unsqueeze(dim=0).cuda())\n",
        "    pred = pred.argmax(dim=1)\n",
        "\n",
        "  # Convert from CHW to HWC for visualization\n",
        "  image = image.permute(1, 2, 0)\n",
        "\n",
        "  # Convert from class indices to class names\n",
        "  pred = dataset[\"test\"].classes[pred]\n",
        "  label = dataset[\"test\"].classes[label]\n",
        "\n",
        "  # Visualize the image\n",
        "  plt.subplot(4, 10, index + 1)\n",
        "  plt.imshow(image)\n",
        "  plt.title(f\"pred: {pred}\" + \"\\n\" + f\"label: {label}\")\n",
        "  plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "inwZEfX3Mo6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d707d1e0"
      },
      "source": [
        "## **Notebook Overview**\n",
        "\n",
        "This notebook provides a tutorial on how to train a neural network using PyTorch, focusing on a variant of the VGG-11 model for the CIFAR-10 dataset. It covers data loading, model definition, training, and evaluation.\n",
        "\n",
        "### 1. Setup\n",
        "This section handles the initial setup for the notebook. It includes:\n",
        "\n",
        "*   **Package Installation**: Installing necessary libraries like `torchprofile` for model analysis.\n",
        "*   **Library Imports**: Importing common PyTorch, torchvision, matplotlib, and numpy modules.\n",
        "*   **Reproducibility**: Setting random seeds for `random`, `numpy`, and `torch` to ensure that results are consistent across runs.\n",
        "\n",
        "### 2. Data\n",
        "This part of the notebook prepares the CIFAR-10 dataset for training and testing.\n",
        "\n",
        "*   **Dataset Definition**: CIFAR-10 is a dataset of 32x32 color images across 10 classes.\n",
        "*   **Transformations**: Data augmentation (random crop, horizontal flip) is applied to the training data, and both training and test data are converted to PyTorch tensors.\n",
        "*   **Visualization**: A code block is provided to visualize a few sample images from the dataset along with their class labels.\n",
        "*   **Data Loaders**: `DataLoader` objects are created for both training and testing datasets, enabling batch processing and shuffling for the training data.\n",
        "\n",
        "### 3. Model\n",
        "This section defines the neural network architecture used in the tutorial.\n",
        "\n",
        "*   **VGG Model**: A custom `VGG` class is defined, which is a variant of the VGG-11 architecture. It consists of a `backbone` (convolutional layers, batch normalization, ReLU, and max-pooling) and a `classifier` (a linear layer).\n",
        "*   **Architecture Details**: The `ARCH` variable specifies the channel sizes and pooling layers. The model processes 3x32x32 images down to 512x2x2 features, which are then flattened and passed to a 10-output linear classifier.\n",
        "*   **Model Inspection**: The notebook prints the structure of the `backbone` and `classifier`.\n",
        "*   **Efficiency Analysis**: It calculates and prints the number of trainable parameters (`num_params`) and the number of multiply–accumulate operations (`num_macs`) using `torchprofile` to assess the model's complexity.\n",
        "\n",
        "\n",
        "### 4. Optimization\n",
        "This section sets up the components required for optimizing the model during training.\n",
        "\n",
        "*   **Loss Function**: `nn.CrossEntropyLoss` is chosen as the criterion, suitable for multi-class classification problems.\n",
        "*   **Optimizer**: `SGD` (Stochastic Gradient Descent) with momentum and weight decay is used to update the model's weights.\n",
        "*   **Learning Rate Scheduler**: A `LambdaLR` scheduler is implemented with a piecewise linear learning rate schedule. This schedule ramps up the learning rate, then gradually decreases it over the training epochs. A plot visualizes this learning rate schedule.\n",
        "\n",
        "### 5. Training\n",
        "This section defines the training and evaluation routines and executes the main training loop.\n",
        "\n",
        "*   **`train` function**: This function performs one epoch of training. It moves data to the GPU, performs forward pass, calculates loss, backpropagates gradients, and updates model parameters using the optimizer and scheduler.\n",
        "*   **`evaluate` function**: This function calculates the accuracy of the model on a given data loader (typically the test set). It operates in `inference_mode` and computes the percentage of correctly classified samples.\n",
        "*   **Training Loop**: The main loop iterates for a specified number of epochs (`num_epochs`). In each epoch, it calls the `train` function and then the `evaluate` function, printing the accuracy at the end of each epoch.\n",
        "\n",
        "### 6. Visualization\n",
        "After training, this final section visualizes the model's predictions.\n",
        "\n",
        "*   **Prediction Visualization**: It iterates through the test dataset, performs inference on individual images, and displays the image along with both the model's prediction and the true label. This helps in qualitatively assessing the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ym2reXFLM0vw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}